print_every: 10
random_seed: 1
platform_config:
  n_agent: 6
  n_task: 2
  n_stacked_observation: 5
  price_bound: 1
  weights_bound: [1 ,5]
  # weights:
  weights: [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]]
  # weights: [[-1, 0], [0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9],[9, 10], [10, 11]]
  task_budget_bound: [40, 41]
  time_budget_bound: [25, 26]
  plot_interval: 1
  penalty_factor: 0.05
  allocation_threshold: 1


agent_config:
  action_size: 1
  obs_size: 2
  mu_cost: 0.05
  action_bound: [0, 1]

train_config:
  result_path: '/results/train_data/'
  model_path: '/results/model/'
  save_every: 500
  n_episodes: 500
  max_t: 30
  mini_batch_size: 128
  update_every: 2
  # optimizers:
  actor_optim_params:
    lr: 0.001
  critic_optim_params:
    lr: 0.001

  # PER
  alpha: 0.4
  beta: 0.6
  beta_increment_per_sampling: 0.002

  # noise
  ou_noise_start: 2.0
  ou_noise_decay_rate: 0.98

  soft_update_tau: 0.01
  discount_gamma: 0.0

  # replay memory (for prioritized replay buffer, the buffer size must be powers of 2)
  buffer_size: 131072
  buffer_mode: 1
  # 0 for random ER, 1 for prioritized ER

  # tensorboard
  len_window: 20

